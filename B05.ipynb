{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFE0Mh-KY4Zu"
   },
   "source": [
    "# Building and Training Neural Networks with Pytorch\n",
    "\n",
    "Pytorch provides built-in functions which make it much easier to build and train neural networks. In this lecture, we will describe how to use these functions to build (i.e. define) and train neural networks. This lecture includes:\n",
    "\n",
    "\n",
    "1. Build a logistic regression model\n",
    "2. Define a training algorithm\n",
    "3. Load a dataset\n",
    "4. Train the logistic regression model\n",
    "5. Test the accuracy of a trained model\n",
    "6. Reading materials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxD-TfGgY4Z4"
   },
   "source": [
    "# Some concepts in machine learning\n",
    "\n",
    "## Training dataset and test dataset\n",
    "Define a dataset as $D = \\{(x_j, y_j)\\}$ and $D=D_1 \\cup D_2$, where:\n",
    "\n",
    "* $D_1$ is the training dataset and $|D_1|$ is the number of data in $D_1$.\n",
    "\n",
    "\n",
    "* $D_2$ is the test dataset and $|D_2|$ is the number of data in $D_2$.\n",
    "\n",
    "\n",
    "\n",
    "## Training loss and test loss\n",
    "Define the loss function:\n",
    "$$L(\\theta) :=\\frac{1}{N} \\sum_{j=1}^N\\ell(y_j, h(x_j; \\theta)).$$\n",
    "Here $\\ell(y_j,h(x_j; \\theta))$ is the  general distance between real label and predicted label. $h(x_j; \\theta)$ is a probability distribution of data $x$.\n",
    "\n",
    "* Training loss is defined as $L(\\theta) :=\\frac{1}{|D_1|} \\sum_{j=1}^{|D_1|}\\ell(y_j, h(x_j; \\theta)).$\n",
    "\n",
    "\n",
    "* Test loss is defined as $L(\\theta) :=\\frac{1}{|D_2|} \\sum_{j=1}^{|D_2|}\\ell(y_j, h(x_j; \\theta)).$\n",
    "\n",
    "\n",
    "## Training accuracy and test accuracy\n",
    "* Training accuracy $= \\frac{\\text{The number of correct classifications in training dataset}}{\\text{the total number of data in training dataset}}$\n",
    "\n",
    "\n",
    "* Test accuracy $= \\frac{\\text{The number of correct classifications in test dataset}}{\\text{the total number of data in test dataset}}$\n",
    "\n",
    "Remark: We usually use the max-out method to do classification. For a given data point $x$, we first compute $h(x;\\theta)$, then we attached $x$ to the class $i= \\arg\\max_j h_j(x; \\theta)$.\n",
    "## Epoch\n",
    "\n",
    "* Epoch: One epoch is when an entire training dataset is used to train the neural network once.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sR6VmmoUY4Z5"
   },
   "source": [
    "# 1. Build a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AeDe2l6NY4Z6"
   },
   "outputs": [],
   "source": [
    "# The most important library for building neural networks is the torch.nn library. \n",
    "# This library allows us to build neural networks by concatenating different types of layers.\n",
    "import torch\n",
    "import torch.nn as nn # Import functions from torch.nn\n",
    "\n",
    "def model(input_size,num_classes):\n",
    "    return nn.Linear(input_size,num_classes) \n",
    "\n",
    "# Define a function named model, which returns a linear model (xW^{T}+b) by nn.Linear(input_size, num_classes)\n",
    "# input_size is the size of the row vector x\n",
    "# num_classes is the number of classes to be classified\n",
    "# This model includes parameters W and b. \n",
    "\n",
    "# Question: what are the sizes of W and b?\n",
    "\n",
    "# Example: in MNIST, the size of all the images is 1*28*28,  \n",
    "# which can be re-arranged to a row vector x with size 784 so that input_size=784\n",
    "# num_classes=10 in MNIST.\n",
    "# Therefore, the size of W is 10*784 and the size of b is 1*10 \n",
    "\n",
    "# Linear model: xW^{T}+b\n",
    "\n",
    "# The size of W is num_classes*input_size\n",
    "# The size of b is 1*num_classes\n",
    "# The size of input x is 1*input_size (if batch_size=1)\n",
    "\n",
    "# Question: what are the sizes of W, x and b if we have a mini-batch of data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Exfon7wvY4Z9"
   },
   "outputs": [],
   "source": [
    "# The cross entropy loss is already implemented in Pytorch. You will see how to use it in section 4\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQY0b-W0Y4Z-"
   },
   "source": [
    "# 2. Define a training algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "id": "ga3Dnv1uY4Z_",
    "outputId": "9bd395b1-08ec-4f86-dc87-a4d99585eeaf"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8cbfb148c368>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# The stochastic gradient descent algorithm with a step size of 0.1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'my_model' is not defined"
     ]
    }
   ],
   "source": [
    "# This library contains implementations of a number of useful optimization algorithms.\n",
    "import torch.optim as optim\n",
    "\n",
    "# The stochastic gradient descent algorithm with a step size of 0.1.\n",
    "optimizer = optim.SGD(my_model.parameters(), lr=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmGvc1-sY4aA"
   },
   "source": [
    "# 3. Load a dataset\n",
    "\n",
    "The MNIST database of handwritten digits, available from this page, has a training dataset of 60,000 examples, and a test dataset of 10,000 examples. The digits have been size-normalized and centered in a fixed-size image ($1*28*28$). (see more on http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "In the following, we show how to load the MNIST dataset by using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEtciFHqY4aB"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Transforms images to a Pytorch Tensor.\n",
    "MNIST_transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "# Load the set of training images. \n",
    "# root is the location of the data you saved\n",
    "# train=True: the data loaded is training data\n",
    "# download=True: if the data does not exist in root, then download it online\n",
    "# transform: do some transformations to the input data\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train= True, download=True, transform=MNIST_transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Load the set of test images.\n",
    "testset = torchvision.datasets.MNIST(root='./data', train= False, download=True, transform=MNIST_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False) \n",
    "\n",
    "# Let’s plot some of the images to see what we’re dealing with.\n",
    "def plot_images(images):\n",
    "    images_for_plot = images[0,0,:,:] # extract the 28*28 tensor from the tensor\n",
    "    print(images_for_plot.size())\n",
    "    plt.imshow(images_for_plot, cmap='gray')  # plot the image with colormaps='gray'\n",
    "    plt.show()\n",
    "\n",
    "for i, (images, labels) in enumerate(trainloader): # ith batch of images and labels\n",
    "    print(images.size())  # [batch_size, channel_size, image_size_x, image_size_y]\n",
    "    plot_images(images)\n",
    "    print('The label of the given image is',labels) \n",
    "    break  #force to stop the for loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiff9m7NY4aD"
   },
   "source": [
    "# 4. Train the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-0gFeNMY4aD"
   },
   "outputs": [],
   "source": [
    "def model(input_size,num_classes):\n",
    "    return nn.Linear(input_size,num_classes)\n",
    "\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "my_model =model(input_size, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(my_model.parameters(), lr=0.1)\n",
    "\n",
    "MNIST_transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train= True, download=True, transform=MNIST_transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train= False, download=True, transform=MNIST_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False) \n",
    "\n",
    "# Write a loop to train the model using the given optimizer and loss functions.\n",
    "num_epochs = 2 \n",
    "# One epoch is when an entire dataset is passed through the neural network only once.\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        # You can check the size of image before images.reshape, which is [batch_size,1,28,28]\n",
    "        #print(images.size()) \n",
    "        # Reshape MNIST images to (batch_size, input_size)\n",
    "        images = images.reshape(images.size(0), 28*28)\n",
    "        # You can check the size of image after images.reshape, which is [batch_size,784]\n",
    "        #print(images.size())\n",
    "        \n",
    "        # Forward pass to get the loss\n",
    "        outputs = my_model(images) # the outputs of xW^{T}+b\n",
    "        loss = criterion(outputs, labels) \n",
    "        \n",
    "        # Backward and compute the gradient\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()  #backpropragation\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0: # \"%\" compute the remainder. print the result every 10000 iterations\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {}' .format(epoch+1, num_epochs, i+1, len(trainloader), loss.item())) \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YR90G-KY4aE"
   },
   "source": [
    "# 5. Test a the accuracy of a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1QocC-ptY4aE"
   },
   "outputs": [],
   "source": [
    "# Calculate the total number of test samples and the number of correctly\n",
    "# classified test samples\n",
    "num_correct = 0\n",
    "total = 0\n",
    "for i, (images, labels) in enumerate(testloader):\n",
    "    images = images.reshape(images.size(0), 1*28*28)\n",
    "    outputs = my_model(images)\n",
    "# Take the most likely label as the predicted label. \n",
    "# value, indices = torch.max(input, dim) \n",
    "# if dim = 0, it returns the maximum value of each column of the given 2D input tensor\n",
    "# if dim = 1, it returns the maximum value of each row of the given 2D input tensor\n",
    "# indices is the index location of each maximum value found (argmax).\n",
    "\n",
    "    p_max, predicted = torch.max(outputs, 1) \n",
    "    total += labels.size(0) # you can also use labels.size()[0], see the test  below\n",
    "    num_correct += (predicted == labels).sum()\n",
    "print('Out of {} samples, the model correctly classified {}, the test accuracy {}' .format(total, num_correct, float(num_correct)/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uTcatrd6Y4aF"
   },
   "outputs": [],
   "source": [
    "# Test torch.max() and tensor.size()\n",
    "a = torch.tensor([[1,2,3,4],[5,6,7,8],[9,14,16,12],[13,10,15,11],[0,0,0,0]])\n",
    "print(a)\n",
    "\n",
    "print(a.size())\n",
    "\n",
    "print(a.size(0))\n",
    "\n",
    "print(a.size()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHNN1mBhY4aG"
   },
   "outputs": [],
   "source": [
    "print(torch.max(a, 1)) # it returns the maximum value of each row of the given 2D input tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJms9_rQY4aG"
   },
   "outputs": [],
   "source": [
    "print(torch.max(a, 0)) # returns the maximum value of each column of the given 2D input tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTGiB5bsY4aH"
   },
   "source": [
    "# Reading material\n",
    "\n",
    "1. (Important) Autograd: Automatic Differentiation https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py\n",
    "\n",
    "2. Classes in Python provide a means of data and functionality together \n",
    "    https://www.w3schools.com/python/python_classes.asp\n",
    "\n",
    "3. Module: a file containing a set of functions you want to include in your application. Consider a module to be the same as a code library.\n",
    "    https://www.w3schools.com/python/python_modules.asp\n",
    "\n",
    "3. Details of torch.nn https://pytorch.org/docs/stable/nn.html\n",
    "\n",
    "4. Details of torch package https://pytorch.org/docs/stable/torch.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qfjmDRBfpd0"
   },
   "source": [
    "# 1.Loading the MNIST dataset in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2VQdywxRhAD-"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5_KgIFZ5gw7V"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "d5jvrWWog0Es"
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsM1Dsg1hUb4"
   },
   "source": [
    "train_images and train_labels form the training set, the data that the model will\n",
    "learn from. The model will then be tested on the test set, test_images and test_labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mar4KVRVhNzo"
   },
   "source": [
    " Let’s look at the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8H1QsRWRhiFV",
    "outputId": "fa68b21d-370f-48dc-cd4a-330a6d1f38e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RQW7Ww9Thkya",
    "outputId": "02d354ad-f82a-48b7-c1cd-21bbf642a303"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJEHf3nNhv2P"
   },
   "source": [
    "And here’s the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8UeuO4J_hnzg",
    "outputId": "480f7aef-9bbf-4e0a-c4bc-e04b1b0fb757"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lK4cF_f6htr-",
    "outputId": "b1c6ac8d-0397-4158-8c40-b84781caa401"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2J3a8och1BF"
   },
   "source": [
    "# 2.  The network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pHmR8nqNh9Yw"
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUd_WXy5iIXx"
   },
   "source": [
    "# 3.The compilation step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqK-CCovlC2c"
   },
   "source": [
    "Gradient descent optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QbgrDxQHiQxf"
   },
   "outputs": [],
   "source": [
    "network.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False, name='SGD'),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdjcfEy1ib5A"
   },
   "source": [
    "# 4.Preparing the image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Ppu3tpVxikAd"
   },
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urZIQp7MimxE"
   },
   "source": [
    "# 5. Preparing the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-atwD_y5irlI"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rq7ijx1Bivz1"
   },
   "source": [
    "# 6.Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OwxqtsGXjD6A",
    "outputId": "dcf27677-ee06-4d3d-920a-7dbb12a26a9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 1.0882 - accuracy: 0.7567\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.5217 - accuracy: 0.8728\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.4204 - accuracy: 0.8901\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.3741 - accuracy: 0.8994\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.3459 - accuracy: 0.9053\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.3256 - accuracy: 0.9103\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.3097 - accuracy: 0.9144\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.2967 - accuracy: 0.9173\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.2855 - accuracy: 0.9207\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.2757 - accuracy: 0.9236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9988d32a90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_images, train_labels, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngox-hCqjGJh"
   },
   "source": [
    "# 7.Check that the model performs well on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AuI7G22Pji7O",
    "outputId": "024c7d7a-af04-44aa-89e3-9794cc29383a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2617 - accuracy: 0.9272\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h4VPlbQCjlKx",
    "outputId": "1edc4c0b-2ccb-484d-c335-4a4063df0256"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.9272000193595886\n"
     ]
    }
   ],
   "source": [
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iv41mzn_jnfv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "B05_MNIST (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
